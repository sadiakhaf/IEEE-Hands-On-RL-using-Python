{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IEEE Hands-On RL using Python Prep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwvowNKJLT5efVJGwcFApy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadiakhaf/IEEE-Hands-On-RL-using-Python/blob/main/IEEE_Hands_On_RL_using_Python_Prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuyVNmxgtwgI"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Setting up the environment with everything we need\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "First things first, lets get us a screen!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLPTNqbMy3td",
        "outputId": "aad1073f-25eb-4cfe-d21a-c34ea7b25132"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "import pyvirtualdisplay\n",
        "display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f571d8d6410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGtCCALYyypV"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Importing Libraries\n",
        "\n",
        "\n",
        "---\n",
        "The most important library we need is *gym*. **gym** provides us **environments**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Computing Libraries\n",
        "\n",
        "\n",
        "\n",
        "Its really up to you whatever library you are comfortable with. **gym** uses NumPy base so you should be able to use it with any library of your choice. I'll use just NumPy. Tensorflow and PyTorch might be your cup of tea so go ahead use them.\n",
        "\n",
        "## Ploting libraries\n",
        "\n",
        "Totally your choice. I like matplotlib so let's use that for today!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTcPlYORy09a"
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "import numpy as np\n",
        "import random #You could just use np.random as well, totally fine\n",
        "from random import randint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC_n50grHQyc"
      },
      "source": [
        "---\n",
        "# Functions of ENVIRONMENT and AGENT\n",
        "---\n",
        "A picture is worth a thousand words!\n",
        "\n",
        "*   What is an **environment** in RL? And What is an **agent**?\n",
        "*   What is *state*?\n",
        "*   What is an *action*?\n",
        "*   What is *reward*?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDhHpNTS_4E9"
      },
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=13oYKs5qWbpPekxMQN5ExG2kLo4ih4pKS)\n",
        "\n",
        "\n",
        "https://drive.google.com/file/d/13oYKs5qWbpPekxMQN5ExG2kLo4ih4pKS/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8EIwx1PvX1e"
      },
      "source": [
        "---\n",
        "# ***THE DRILL***\n",
        "---\n",
        "\n",
        "\n",
        "In the *beginning* there was nothing. then came \"initial state\"\n",
        "\n",
        "1. REST() env and get state\n",
        "  2.   Give this state to *agent*, wait for him to *act* --> ACT()\n",
        "  3.   Give his *action* to env and get *reward* --> STEP()\n",
        "  4.   Pass this reward to *agent* for his behavior, make him learn --> UPDATE(). Plot something if you need to.\n",
        "  5.   Go to step 2\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpsMdeiOuMl8"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Importing you first **gym** environment\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*   What is this **gym** anyway?\n",
        "*   Why should I use gym? My work has nothing to do with *games*.\n",
        "*   Where do I find documentation and help for gym?\n",
        "*   What exactly are *spaces* in gym?\n",
        "\n",
        "# Complete gym codes\n",
        "Go check it out on github: https://github.com/openai/gym/tree/master/gym \n",
        "\n",
        "# More gym environments for you to try\n",
        "More OpenAI gym environments can be found on OpenAI gym website. \n",
        "https://gym.openai.com/envs/#classic_control \n",
        "\n",
        "---\n",
        "# How do I install gym on my local machine, I'm using Visual Studio Code and Anaconda\n",
        "*    Create  new environment e.g. NumPygymEnv\n",
        "*    Activate the newly created environment\n",
        "*    pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3eV0EZwy8Lj"
      },
      "source": [
        "env_name = \"CartPole-v1\"\n",
        "# env_name = \"MountainCar-v0\"\n",
        "# env_name = \"Pendulum-v0\"\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50):\n",
        "  action = env.action_space.sample()\n",
        "  # action = 1\n",
        "  print(\"The selected action is: \",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "  \n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "    \n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zhl2r6azAGi"
      },
      "source": [
        "# display.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQVBHI25IO5a"
      },
      "source": [
        "---\n",
        "# But how do I know what are the inputs and outputs of these functions??\n",
        "---\n",
        "\n",
        "1.   help()\n",
        "2.   dir()\n",
        "3.   Go to the source code -> https://github.com/openai/gym/tree/master/gym \n",
        "4.   If you still need help, google it\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_s0gV2UHtBx",
        "outputId": "593e2f72-4908-45b2-bf36-8ff520c15072"
      },
      "source": [
        "help(env.step)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method step in module gym.wrappers.time_limit:\n",
            "\n",
            "step(action) method of gym.wrappers.time_limit.TimeLimit instance\n",
            "    Run one timestep of the environment's dynamics. When end of\n",
            "    episode is reached, you are responsible for calling `reset()`\n",
            "    to reset this environment's state.\n",
            "    \n",
            "    Accepts an action and returns a tuple (observation, reward, done, info).\n",
            "    \n",
            "    Args:\n",
            "        action (object): an action provided by the agent\n",
            "    \n",
            "    Returns:\n",
            "        observation (object): agent's observation of the current environment\n",
            "        reward (float) : amount of reward returned after previous action\n",
            "        done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
            "        info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR3QtlXWj46Z"
      },
      "source": [
        "?env.observation_space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GctfCyJ0Jmdj"
      },
      "source": [
        "dir(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeZLG-vEt-gH"
      },
      "source": [
        "---\n",
        "## What if I need to do environment specific tasks in my code?\n",
        "---\n",
        " Can I get an environment name or id from the created env?\n",
        "\n",
        " \n",
        "Yes you can. Lets google it. Head over to google search and type \"gym get env name\". Go thought first 2,3 links. Let me know what you found?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csZ7-tOWKFU9",
        "outputId": "8bfce1ff-d0a2-4be5-da68-0ff3655b71c4"
      },
      "source": [
        "print(\"Environment name: \",env.unwrapped.spec.id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Environment name:  CartPole-v1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rywyKEyOK7L-",
        "outputId": "c93d1603-3033-44cf-f13a-6c73b94ef625"
      },
      "source": [
        "if \"CartPole\" in env.unwrapped.spec.id:\n",
        "  print(\"Env is CartPole Env\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Env is CartPole Env\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__wAyzNNvHWn"
      },
      "source": [
        "---\n",
        "# Let's write our **AGENT** now!!\n",
        "---\n",
        "What does an agent do?\n",
        "\n",
        "What functions did we say an agent will have?\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goaXVo5CNRT9"
      },
      "source": [
        "---\n",
        "# Q-table update\n",
        "---\n",
        "\n",
        "From Sutton and Barto Book:\n",
        "\n",
        "$Q(S_t,A_t) \\leftarrow Q(s_t,A_t) + \\alpha\\left[R_{t+1} + \\gamma \\max_a {Q(S_{t+1},a)}-Q(S_t,A_t)\\right]$\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDh3IO6YKrnb"
      },
      "source": [
        "class Agent:\n",
        "  def __init__(\n",
        "    self, lr: float, gamma: float, act_space: spaces.Discrete,\n",
        "    obs_space: spaces.Discrete, num_episodes: int, obs_space_len , action_space_len\n",
        "  ):\n",
        "\n",
        "    self.lr = lr  # learning rate\n",
        "    self.gamma = gamma  # gamma parameter\n",
        "    self.act_space = act_space\n",
        "    self.obs_space = obs_space\n",
        "    self.obs_space_len = obs_space_len\n",
        "    self.action_space_len = action_space_len\n",
        "    self.num_episodes = num_episodes  # episodes\n",
        "    self.eps: Array = np.exp(-5*np.linspace(0,1,num_episodes))   # epsilon value\n",
        "    self.qtable = np.random.uniform(low=-2, high=0, size=(self.obs_space_len+[self.action_space_len])) # Q-table\n",
        "    \n",
        "\n",
        "  def act(self, o: int, episode_i: int) -> int:  # function to choose action\n",
        "\n",
        "    if random.uniform(1.0, 0.0) < self.eps[episode_i]:  # epsilon-greedy condition for exploration\n",
        "      return # a uniformly sampled random action from action_space\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return #the action that has the highest Q-value in state/observation o\n",
        "\n",
        "\n",
        "  def update(self, o: int, a: int, r: float, o_prime: int) -> float:  # agent update function (e.g. Q-learning update)\n",
        "\n",
        "    old_o_a_value   #store the Q-value for observation o and action a \n",
        "    q_prime   # estimate of optiomal future value (maximum Q-value in observation o_prime)\n",
        "\n",
        "    # Update Q-table based on the equation above\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    return  # return delta update to training loop\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vd6cYpKy-Fv"
      },
      "source": [
        "---\n",
        "For some environments we make small adjustments, like discretizing the states.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsmagt3gyDhg"
      },
      "source": [
        "  def get_discrete_state(state,obs_space_len,bins):\n",
        "    stateIndex = []\n",
        "    for i in range(obs_space_len):\n",
        "      stateIndex.append(np.digitize(state[i], bins[i]) - 1) # -1 will turn bin into index\n",
        "    return tuple(stateIndex) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR2D_lf4zMBA"
      },
      "source": [
        "---\n",
        "Let's see if our newly created agent is behaving the way it is supposed to.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUJHplpDSwzC"
      },
      "source": [
        "agent = Agent(lr=0.1, gamma=0.95, act_space=env.action_space, \n",
        "              obs_space=env.observation_space, num_episodes=10000, \n",
        "              obs_space_len=[20], action_space_len=2)\n",
        "print(agent.qtable.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRFE_7_lRC_f"
      },
      "source": [
        "\n",
        "---\n",
        "# The main loop for training the agent\n",
        "---\n",
        "\n",
        "Here is where all the action happens!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9AA4dJNRLiy"
      },
      "source": [
        "import pdb\n",
        "random.seed(1234)  # python random number generator seed\n",
        "DISPLAY_EVERY = 100\n",
        "if \"CartPole\" in env_name:\n",
        "  # print(\"Env is CartPole Env\")\n",
        "  \n",
        "  num_bins = 20\n",
        "  bins = [\n",
        "          np.linspace(-4.8, 4.8, num_bins),\n",
        "          np.linspace(-4, 4, num_bins),\n",
        "          np.linspace(-.418, .418, num_bins),\n",
        "          np.linspace(-4, 4, num_bins)\n",
        "        ]\n",
        "\n",
        "lr = 0.1  # learning rate\n",
        "gamma = 0.95  # gamma parameter\n",
        "num_episodes = 10000  # number of steps (episodes) in epsilon log-space\n",
        "# initialize our Rx agent with learning parameters and gym parameters\n",
        "if \"CartPole\" in env_name:\n",
        "  obs_space_len = [num_bins+1] *len(env.observation_space.high)\n",
        "  action_space_len = env.action_space.n\n",
        "  agent = Agent(\n",
        "      lr=lr,gamma=gamma,act_space=env.action_space,obs_space=env.observation_space,\n",
        "      num_episodes=num_episodes,obs_space_len=obs_space_len, action_space_len=action_space_len\n",
        "      )\n",
        "  prev_screen = env.render(mode='rgb_array')\n",
        "  plt.imshow(prev_screen)\n",
        "else:\n",
        "  obs_space_len = [env.horizon+1]\n",
        "  action_space_len = 2\n",
        "  agent = Agent(\n",
        "      lr=lr,gamma=gamma,act_space=env.action_space,obs_space=env.observation_space,\n",
        "      num_episodes=num_episodes,obs_space_len=obs_space_len, action_space_len=action_space_len\n",
        "      )  \n",
        "# show_verbose = True  # print information for debugging\n",
        "show_verbose = False\n",
        "render_gym = False  # render gym\n",
        "# render_gym = True\n",
        "running_len = 5  # length of our running training data\n",
        "running_delta = []  # running delta (e.g. the last running_len delta update)\n",
        "running_acc = []  # running accuracy (e.g. the last running_len accuracy)\n",
        "\n",
        "\n",
        "for episode_i in range(num_episodes): # episode loop\n",
        "  step_counter = 1\n",
        "  done = False\n",
        "  if show_verbose:\n",
        "    print('starting episode {episode_i}...'.format(episode_i=episode_i))\n",
        "  delta_update = []  # delta update of our Q-table\n",
        "  n_successes: int = 0  # number of optimal actions (actions with maximum reward)\n",
        "  cumul_r: float = 0.0  # cumulative reward\n",
        "  ############### Reset the environment and get initial o ##########################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if \"CartPole\" in env_name:\n",
        "    o = get_discrete_state(o,obs_space_len=len(env.observation_space.high),bins=bins)\n",
        "  while not done:\n",
        "    ################ Call the act function from agent and get the action for current o and episode_i ####################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ################ Pass this action to what function now? ############### What to get from that function? #############\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if \"CartPole\" in env_name:\n",
        "      o_prime = get_discrete_state(o_prime,len(env.observation_space.high),bins=bins)\n",
        "\n",
        "    delta_update.append(agent.update(o=o,a=a,r=r,o_prime=o_prime))  # update agent with transition, get delta update\n",
        "\n",
        "    ######################## Update the state ##################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    cumul_r += r  # add reward to cumulative reward\n",
        "    n_successes += int(r > 0.0)  # success if optimal action-reward of 1.0\n",
        "    step_counter += 1\n",
        "    if episode_i % DISPLAY_EVERY == 0:\n",
        "      if render_gym:\n",
        "        if \"CartPole\" in env_name:\n",
        "          screen = env.render(mode='rgb_array')\n",
        "          plt.imshow(screen)\n",
        "          ipythondisplay.clear_output(wait=True)\n",
        "          ipythondisplay.display(plt.gcf())\n",
        "      if show_verbose:   # show transition of our model (e.i. <o, a, r, o'>)\n",
        "        print(\n",
        "          'transition=<{o}, {a}, {r}, {o_prime}>,'\n",
        "          ' delta_update={delta}'.format(\n",
        "            o=o, a=a, r=r, o_prime=o_prime, delta=delta_update[-1]\n",
        "          )\n",
        "        )    \n",
        "\n",
        "  running_acc.append(n_successes / step_counter)  # add latest accuracy to running data\n",
        "  running_delta.append(sum(delta_update).item())  # add latest update delta to running data\n",
        "  # print(running_delta)\n",
        "  print(\n",
        "    'episode {episode_i}: cumul_reward={cumul_r}, accuracy:{acc:0.1}, '\n",
        "    'cumul_delta={cumul_delta:0.1}, eps={eps:0.1}'.format(\n",
        "      episode_i=episode_i, cumul_r=cumul_r, acc=running_acc[-1],\n",
        "      cumul_delta=running_delta[episode_i], eps=agent.eps[episode_i]\n",
        "    )\n",
        "  )\n",
        "\n",
        "  # training stop conditions\n",
        "  if (\n",
        "    all([acc == 1.0 for acc in running_acc])  # all running accuracy are maximized\n",
        "    and all([delta < 0.0001 for delta in running_delta])  # all running delta update a lower than 0.0001\n",
        "    and episode_i >= running_len  # running data have reached running length\n",
        "  ) or episode_i >= num_episodes + running_len:  # epsilon was 0 for all running data (nothing will change)\n",
        "    break  # exit training loop\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()  # close gym environment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhlyAhmYTNJF",
        "outputId": "82d9c476-a42d-4080-ed44-681f974f6639"
      },
      "source": [
        "display.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f571d8d6410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGzrIt8G49h2"
      },
      "source": [
        "stepsize= round(num_episodes/20)\n",
        "stdfigsize=(3.51*1.5,3*1.5)\n",
        "x = np.arange(1,num_episodes,stepsize)\n",
        "mkrs = [\"C2-s\",\"C0-o\",\"C1-^\",\"C4-*\",\"C3-d\",\"C4-x\",\"C5-.\",\"C6-d\",\"C7-v\",\"C8-,\",\"C9-<\",\"C20\",\"C11-\",\"C12-\"]\n",
        "fig = plt.figure(figsize=stdfigsize)\n",
        "\n",
        "plt.plot(x, running_acc[1:-1:stepsize],mkrs[0],label='Accuracy')\n",
        "plt.xlabel('Episode Number', fontsize='large')\n",
        "plt.ylabel('Accuracy', fontsize='large')\n",
        "plt.legend(loc='lower right',shadow=True,fontsize='large')\n",
        "plt.grid(True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuycgQ89m1NM"
      },
      "source": [
        "stepsize= round(num_episodes/20)\n",
        "stdfigsize=(3.51*1.5,3*1.5)\n",
        "x = np.arange(1,num_episodes,stepsize)\n",
        "mkrs = [\"C2-s\",\"C0-o\",\"C1-^\",\"C4-*\",\"C3-d\",\"C4-x\",\"C5-.\",\"C6-d\",\"C7-v\",\"C8-,\",\"C9-<\",\"C20\",\"C11-\",\"C12-\"]\n",
        "fig = plt.figure(figsize=stdfigsize)\n",
        "plt.plot(x, running_delta[1:-1:stepsize],mkrs[4],label='Delta')\n",
        "plt.xlabel('Episode Number', fontsize='large')\n",
        "plt.ylabel('Accuracy', fontsize='large')\n",
        "plt.legend(loc='upper right',shadow=True,fontsize='large')\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa3EXI7nm15Y"
      },
      "source": [
        "# Great Stuff but I'm not here to play GAMES!!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "What if I want to use RL for **REAL** research?\n",
        "\n",
        "\n",
        "*   Wireless communications\n",
        "*   Control systems\n",
        "*   Biomedical engineering\n",
        "*   Informatics\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### How do I make RL useful for my **WORK**\n",
        "Good News, you CAN!\n",
        "\n",
        "Here's how:\n",
        "Remember the **DRILL**??\n",
        "\n",
        "\n",
        "1.   Write your own *Environment*\n",
        "2.   Modify the *Agent* if your need to\n",
        "3.   Run the training loop (do I need to store any additional permonace metric??)\n",
        "4.   Plot your results\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FKabvf87A49"
      },
      "source": [
        "class WireLessEnv(gym.Env):\n",
        "  def __init__(self, n_PU =1 , horizon =20):\n",
        "    super(WireLessEnv, self).__init__()  # initialize gym.Env base class\n",
        "    self.action_space: spaces.MultiBinary = spaces.Box(low=0,high=1,shape=(n_PU,),dtype=np.int32)\n",
        "    self.observation_space: spaces.Discrete = spaces.Discrete(horizon + 1)  # observation space {0, 1, ..., horizon}\n",
        "    self.horizon = horizon  # gym horizon to know when we are DONE\n",
        "    self.n_PU = n_PU #Number of PUs\n",
        "    self.TxPattern = np.random.randint(2, size=(horizon,n_PU))\n",
        "    self.t = 0  # initial time-step / observation \n",
        "    \n",
        "  \n",
        "\n",
        "  def reset(self): # Returns initial state of environment\n",
        "    self.t = 0\n",
        "    ############### Return initial state of the environment #########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return \n",
        "\n",
        "\n",
        "\n",
        "  def step(self, action): #action is a binary  0 or 1\n",
        "\n",
        "    if self.t < self.horizon:  # non-terminal observation, horizon not reached\n",
        "      ### Calculate Reward\n",
        "      r = np.sum((action == self.TxPattern[self.t,:]).astype(int)) \n",
        "\n",
        "    else:  # gym horizon reached\n",
        "      r = 0.0\n",
        "\n",
        "    self.t += 1  # increment our time-step / observation\n",
        "    o = self.t  # observation that will return\n",
        "    done = (self.t == self.horizon)  # is terminal gym state reached\n",
        "\n",
        "    return o, r, done, {}  # gyms always returns <obs, reward, terminal obs reached, debug/info dictionary>\n",
        "\n",
        "  def render(self,mode='human'):\n",
        "    print('Lets see TxPattern \\n')\n",
        "    print(self.TxPattern)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKjCNvzQ8Cc1"
      },
      "source": [
        "---\n",
        "# Let's get our env\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlU__RFy8BWn"
      },
      "source": [
        "env_name = \"WirelessEnv\"\n",
        "env = WireLessEnv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqvC7fGSqwPw"
      },
      "source": [
        "dir(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucveVNtFsfEs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4cbKxce7gcO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u4oPesHntlK"
      },
      "source": [
        "stepsize= round(num_episodes/20)\n",
        "stdfigsize=(3.51*1.5,3*1.5)\n",
        "x = np.arange(1,num_episodes,stepsize)\n",
        "mkrs = [\"C2-s\",\"C0-o\",\"C1-^\",\"C4-*\",\"C3-d\",\"C4-x\",\"C5-.\",\"C6-d\",\"C7-v\",\"C8-,\",\"C9-<\",\"C20\",\"C11-\",\"C12-\"]\n",
        "fig = plt.figure(figsize=stdfigsize)\n",
        "plt.plot(x, running_acc[1:-1:stepsize],mkrs[0],label='Accuracy')\n",
        "plt.xlabel('Episode Number', fontsize='large')\n",
        "plt.ylabel('Accuracy', fontsize='large')\n",
        "plt.legend(loc='lower right',shadow=True,fontsize='large')\n",
        "plt.grid(True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PakLx_W1wMeK"
      },
      "source": [
        "stepsize= round(num_episodes/20)\n",
        "stdfigsize=(3.51*1.5,3*1.5)\n",
        "x = np.arange(1,num_episodes,stepsize)\n",
        "mkrs = [\"C2-s\",\"C0-o\",\"C1-^\",\"C4-*\",\"C3-d\",\"C4-x\",\"C5-.\",\"C6-d\",\"C7-v\",\"C8-,\",\"C9-<\",\"C20\",\"C11-\",\"C12-\"]\n",
        "fig = plt.figure(figsize=stdfigsize)\n",
        "plt.plot(x, running_delta[1:-1:stepsize],mkrs[4],label='Delta')\n",
        "plt.xlabel('Episode Number', fontsize='large')\n",
        "plt.ylabel('Accuracy', fontsize='large')\n",
        "plt.legend(loc='upper right',shadow=True,fontsize='large')\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ren98xUpyDxA"
      },
      "source": [
        "---\n",
        "# What was Epsilon?\n",
        "---\n",
        "There is something called exploration vs. expoloitation in RL.\n",
        "\n",
        "\n",
        "> *Should the agent try a new action, or the action that gave the best reward previously?*\n",
        "\n",
        "A dog may have gotten a reward for spinning around but should it try fetching the ball in hopes of getting better reward?\n",
        "\n",
        "*   Try the best action so far (greedy action) with probability $1-\\epsilon$\n",
        "*  Try a random action from action-space with probability $\\epsilon$\n",
        "*   Should this probability $\\epsilon$ be constant thoughout the training?\n",
        "\n",
        "There are other ways to balance explorartion vs. exploitation e.g. Upper Confidence Bounds (UCB)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk9Tf8mrTSnG"
      },
      "source": [
        "\n",
        "stepsize= round(num_episodes/20)\n",
        "stdfigsize=(3.51*1.5,3*1.5)\n",
        "x = np.arange(1,num_episodes,stepsize)\n",
        "mkrs = [\"C2-s\",\"C0-o\",\"C1-^\",\"C4-*\",\"C3-d\",\"C4-x\",\"C5-.\",\"C6-d\",\"C7-v\",\"C8-,\",\"C9-<\",\"C20\",\"C11-\",\"C12-\"]\n",
        "fig = plt.figure(figsize=stdfigsize)\n",
        "plt.plot(x, agent.eps[1:-1:stepsize],mkrs[2],label='$\\epsilon$')\n",
        "plt.xlabel('Episode Number', fontsize='large')\n",
        "plt.ylabel('$\\epsilon$', fontsize='large')\n",
        "plt.legend(loc='upper right',shadow=True,fontsize='large')\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}